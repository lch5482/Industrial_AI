{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13d109-b972-429c-94c9-55dd7f37f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def ngrams(sentence, n):\n",
    "    words = sentence.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return list(ngrams)\n",
    "\n",
    "sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n",
    "\n",
    "unigram = ngrams(sentence, 1)\n",
    "bigram = ngrams(sentence, 2)\n",
    "trigram = ngrams(sentence, 3)\n",
    "\n",
    "print(unigram)\n",
    "print(bigram)\n",
    "print(trigram)\n",
    "\n",
    "unigram = nltk.ngrams(sentence.split(), 1)\n",
    "bigram = nltk.ngrams(sentence.split(), 2)\n",
    "trigram = nltk.ngrams(sentence.split(), 3)\n",
    "\n",
    "print(list(unigram))\n",
    "print(list(bigram))\n",
    "print(list(trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a00f1-bdec-441d-bc13-121e0c296410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"That movie is famous movie\",\n",
    "    \"I like that actor\",\n",
    "    \"I don’t like that actor\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3756f-93ef-4d7b-a61c-70e60a689760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class VanillaSkipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output = self.linear(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452e944-99e4-4584-b6a0-88ddc77cf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242cddc-b96e-4cc3-8b14-6dd61a1e48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5be65c-a2c7-4a0a-b260-c69c5e27bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start = max(0, idx - window_size)\n",
    "            window_end = min(sentence_length, idx + window_size + 1)\n",
    "            center_word = sentence[idx]\n",
    "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "word_pairs = get_word_pairs(tokens, window_size=2)\n",
    "print(word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3bb47-37b8-456a-8371-d1faaa4eda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = []\n",
    "    unk_index = token_to_id[\"<unk>\"]\n",
    "    for word_pair in word_pairs:\n",
    "        center_word, context_word = word_pair\n",
    "        center_index = token_to_id.get(center_word, unk_index)\n",
    "        context_index = token_to_id.get(context_word, unk_index)\n",
    "        pairs.append([center_index, context_index])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
    "print(index_pairs[:5])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915d51c-6440-47e5-af3b-40c57923fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "index_pairs = torch.tensor(index_pairs)\n",
    "center_indexes = index_pairs[:, 0]\n",
    "context_indexes = index_pairs[:, 1]\n",
    "\n",
    "dataset = TensorDataset(center_indexes, context_indexes)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa634889-1672-48d4-b0d8-a8758f03d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ab10e-517c-497f-99c3-7ad5058f2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    cost = 0.0\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        logits = word2vec(input_ids)\n",
    "        loss = criterion(logits, target_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee517a-ea9f-4feb-8eac-b57a28a3db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding\n",
    "\n",
    "index = 30\n",
    "token = vocab[index]\n",
    "token_embedding = token_to_embedding[token]\n",
    "print(token)\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea695e-8c70-4736-a427-b61322011344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
    "    return cosine\n",
    "\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1 : n + 1]\n",
    "    return top_n\n",
    "\n",
    "\n",
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=5)\n",
    "\n",
    "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd387120",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=tokens,\n",
    "    vector_size=128,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    epochs=3,\n",
    "    max_final_vocab=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b83369",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save(\"../models/word2vec.model\")\n",
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"연기\"\n",
    "print(word2vec.wv[word])\n",
    "print(word2vec.wv.most_similar(word, topn=5))\n",
    "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e63add-f91e-421b-ad32-531ed802289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"kornli\")\n",
    "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
    "tokens = [sentence.split() for sentence in corpus_texts]\n",
    "\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f20efe-1a33-4dc6-89e5-f66bf5c2e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "fastText = FastText(\n",
    "    sentences=tokens,\n",
    "    vector_size=128,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    max_final_vocab=20000,\n",
    "    epochs=3,\n",
    "    min_n=2,\n",
    "    max_n=6\n",
    ")\n",
    "\n",
    "# fastText.save(\"../models/fastText.model\")\n",
    "# fastText = FastText.load(\"../models/fastText.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92edfb-febe-4ed3-a083-4d06122355ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = \"사랑해요\"\n",
    "oov_vector = fastText.wv[oov_token]\n",
    "\n",
    "print(oov_token in fastText.wv.index_to_key)\n",
    "print(fastText.wv.most_similar(oov_vector, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5a81c-54be-4c00-9378-4be0122ba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "input_size = 128\n",
    "ouput_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "\n",
    "model = nn.RNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=ouput_size,\n",
    "    num_layers=num_layers,\n",
    "    nonlinearity=\"tanh\",\n",
    "    batch_first=True,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "sequence_len = 6\n",
    "\n",
    "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
    "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
    "\n",
    "outputs, hidden = model(inputs, h_0)\n",
    "print(outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d7c08-8d28-45d0-887a-a63b6dea7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "input_size = 128\n",
    "output_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "proj_size = 64\n",
    "\n",
    "model = nn.LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=bidirectional,\n",
    "    proj_size=proj_size,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "sequence_len = 6\n",
    "\n",
    "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
    "h_0 = torch.rand(\n",
    "    num_layers * (int(bidirectional) + 1),\n",
    "    batch_size,\n",
    "    proj_size if proj_size > 0 else output_size,\n",
    ")\n",
    "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
    "\n",
    "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
    "\n",
    "print(outputs.shape)\n",
    "print(h_n.shape)\n",
    "print(c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f61d2a-a53b-4e37-b118-927bb59d346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71519fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1c0ab-dd85-4f97-b013-c1e180141f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b6525-cd9a-4d29-a116-05936a1e92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ad18f-31e7-4480-8460-d0b3e0c8ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994b8de-7656-42b9-b191-9bceaddd9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9227be-a546-4157-b472-c6d66d6897ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ee60e-31ca-4722-85d1-00c1d48bc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3de32-698c-41c0-bf6e-4899cee38f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\",\n",
    "        pretrained_embedding=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=n_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                padding_idx=0\n",
    "            )\n",
    "        \n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f220c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cffa5-5e60-47cf-bb4e-73815ed235ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7647511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0247c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16729166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13af291",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8369d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc439ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, \n",
    "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5776758-c4df-4bce-940f-f161561d0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
