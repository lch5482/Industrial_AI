{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b53dc-ba59-48ab-9276-cf1d5b24c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \" 현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "tokenized = review.split()\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "tokenized = list(review)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c3102-d005-48dc-bb20-56756c989353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj\n",
    "\n",
    "\n",
    "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
    "decomposed = j2hcj(h2j(review))\n",
    "tokenized = list(decomposed)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00deb409-9bfc-44c5-9f02-745b75a786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
    "\n",
    "nouns = okt.nouns(sentence)\n",
    "phrases = okt.phrases(sentence)\n",
    "morphs = okt.morphs(sentence)\n",
    "pos = okt.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"구 추출 :\", phrases)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
    "\n",
    "nouns = kkma.nouns(sentence)\n",
    "sentences = kkma.sentences(sentence)\n",
    "morphs = kkma.morphs(sentence)\n",
    "pos = kkma.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"문장 추출 :\", sentences)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de2b90-4221-4d6b-8011-a00b7361575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a31b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "\n",
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "sent_tokens = tokenize.sent_tokenize(sentence)\n",
    "\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f146722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tag\n",
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "\n",
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "pos = tag.pos_tag(word_tokens)\n",
    "\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245492d6-02c8-4444-a878-1480ed0fa34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d6d57-69dd-41e8-9deb-46adcb8498c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "dataset = corpus.train\n",
    "petition = dataset[0]\n",
    "\n",
    "print(\"청원 시작일 :\", petition.begin)\n",
    "print(\"청원 종료일 :\", petition.end)\n",
    "print(\"청원 동의 수 :\", petition.num_agree)\n",
    "print(\"청원 범주 :\", petition.category)\n",
    "print(\"청원 제목 :\", petition.title)\n",
    "print(\"청원 본문 :\", petition.text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "petitions = corpus.get_all_texts()\n",
    "with open(\"../datasets/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for petition in petitions:\n",
    "        f.write(petition + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613f94b-4bad-4962-8f6f-8344972168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "\n",
    "SentencePieceTrainer.Train(\n",
    "    \"--input=../datasets/corpus.txt\\\n",
    "    --model_prefix=../models/petition_bpe\\\n",
    "    --vocab_size=8000 model_type=bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e188c9-da48-4968-8d56-d17f365ae03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"../models/petition_bpe.model\")\n",
    "\n",
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
    "\n",
    "tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n",
    "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
    "print(\"단일 문장 토큰화 :\", tokenized_sentence)\n",
    "print(\"여러 문장 토큰화 :\", tokenized_sentences)\n",
    "\n",
    "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
    "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
    "print(\"단일 문장 정수 인코딩 :\", encoded_sentence)\n",
    "print(\"여러 문장 정수 인코딩 :\", encoded_sentences)\n",
    "\n",
    "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
    "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
    "print(\"정수 인코딩에서 문장 변환 :\", decode_ids)\n",
    "print(\"하위 단어 토큰에서 문장 변환 :\", decode_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31ca0b-05de-471b-9c10-eb57bef25ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"../models/petition_bpe.model\")\n",
    "\n",
    "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
    "print(list(vocab.items())[:5])\n",
    "print(\"vocab size :\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520f0f2-d4d9-4660-9751-5cfc2726f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train([\"../datasets/corpus.txt\"])\n",
    "tokenizer.save(\"../models/petition_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bee282-8abe-4914-9b4b-efe9d737e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"../models/petition_wordpiece.json\")\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
    "\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "encoded_sentences = tokenizer.encode_batch(sentences)\n",
    "\n",
    "print(\"인코더 형식 :\", type(encoded_sentence))\n",
    "\n",
    "print(\"단일 문장 토큰화 :\", encoded_sentence.tokens)\n",
    "print(\"여러 문장 토큰화 :\", [enc.tokens for enc in encoded_sentences])\n",
    "\n",
    "print(\"단일 문장 정수 인코딩 :\", encoded_sentence.ids)\n",
    "print(\"여러 문장 정수 인코딩 :\", [enc.ids for enc in encoded_sentences])\n",
    "\n",
    "print(\"정수 인코딩에서 문장 변환 :\", tokenizer.decode(encoded_sentence.ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
